{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.4"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python394jvsc74a57bd041bdca2920437d6fde7be0e203db1917f26fd3cb8f6aea7086d1780520d6fe4a",
   "display_name": "Python 3.9.4 64-bit ('PythonSoftwareFoundation.Python.3.9_qbz5n2kfra8p0')"
  },
  "metadata": {
   "interpreter": {
    "hash": "41bdca2920437d6fde7be0e203db1917f26fd3cb8f6aea7086d1780520d6fe4a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "# VSM \"ONe time RUn \""
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "[nltk_data] Downloading package wordnet to\n",
      "[nltk_data]     C:\\Users\\LEO\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "StopWordsRemoval->\n",
      "Okey\n",
      "lemmatization->\n",
      "Okey\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"VSM.ipynb\n",
    "\n",
    "Automatically generated by Colaboratory.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1T4deHJMx122ybXYUXVTZlwhIbG3bAAYo\n",
    "\n",
    "## PreProcessing\n",
    "\"\"\"\n",
    "\n",
    "def tokenization_caseFoldingg() :\n",
    "    noDoc = 50\n",
    "    tokens = {}\n",
    "\n",
    "    for i in range(noDoc) :\n",
    "        myfile = open('ShortStories\\{0}.txt'.format(i+1), encoding='utf-8')\n",
    "        string  = myfile.read()\n",
    "        string.strip()\n",
    "        re.sub('[^A-Za-z0-9]+', '',string)\n",
    "        dictt = {}\n",
    "        dictt = invertedIndexx(string.lower(),i+1)\n",
    "        myfile.close()\n",
    "        for x,y in dictt.items() :\n",
    "           if tokens.get(x) == None:\n",
    "                tokens.update({x:[]})\n",
    "           tokens[x].extend(y)\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "def StopWordsRemoval(token):\n",
    "    myfile = open('Stopword-List.txt',encoding='utf-8')\n",
    "    string = myfile.read()\n",
    "    dictt = {}\n",
    "    for x,y in token.items() :\n",
    "      for yy in y :\n",
    "        if yy not in string.split() :\n",
    "            if dictt.get(x) == None :\n",
    "                dictt.update({x:[]})\n",
    "            dictt[x].append(yy)\n",
    "    myfile.close()\n",
    "    return dictt\n",
    "\n",
    "def invertedIndexx(string,j): \n",
    "    s=''\n",
    "    tokens = {}\n",
    "    for i in range(len(string)) :\n",
    "        if string[i] != ' ' and string[i].isalnum():\n",
    "            s=s+string[i]\n",
    "        elif s!= '' :\n",
    "                \n",
    "            if tokens.get(j) == None :\n",
    "                    tokens.update({j:[]})\n",
    "            tokens[j].append(s.lower())\n",
    "            s=''\n",
    "    \n",
    "    return tokens\n",
    "\n",
    "import nltk \n",
    "def stemming(token) :\n",
    "\n",
    "  from nltk.stem import PorterStemmer\n",
    "  porter = PorterStemmer()\n",
    "\n",
    "  temp = {}\n",
    "  for docid,words in token.items() :\n",
    "    for word in words :\n",
    "      newword = porter.stem(''.join(map(str, word)))\n",
    "      if temp.get(docid) == None :\n",
    "                  temp.update({docid:[]})\n",
    "      \n",
    "      temp[docid].append(newword)\n",
    "\n",
    "  \n",
    "\n",
    "  return temp\n",
    "\n",
    "import nltk\n",
    "nltk.download('wordnet')\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "def lemmatization(token):\n",
    " \n",
    "  \n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  # newword = wordnet_lemmatizer.lemmatize(\"better\")\n",
    "  # print(newword)\n",
    "  temp = {}\n",
    "  for docid,words in token.items() :\n",
    "    for word in words :\n",
    "      newword = wordnet_lemmatizer.lemmatize(''.join(map(str, word)))\n",
    "      if temp.get(docid) == None :\n",
    "                  temp.update({docid:[]})\n",
    "      \n",
    "      temp[docid].append(newword)\n",
    "\n",
    "  \n",
    "\n",
    "  return temp\n",
    "\n",
    "def unique(list1,unique_list):\n",
    " \n",
    "     \n",
    "    # traverse for all elements\n",
    "    for x in list1:\n",
    "        # check if exists in unique_list or not\n",
    "        if x not in unique_list:\n",
    "            unique_list.append(x)\n",
    "    # print list\n",
    "    \n",
    "    return unique_list\n",
    "\n",
    "\"\"\"## save in DISK\"\"\"\n",
    "\n",
    "def save(invertedIndex,name) :\n",
    "  import json \n",
    "  with open('GUI\\{0}.json'.format(name), 'w') as ij:\n",
    "    json.dump(invertedIndex,ij)\n",
    "\n",
    "  ij.close();\n",
    "\n",
    "\"\"\"## MAIN (pre and save)\"\"\"\n",
    "\n",
    "def sortIndex(tokens) :\n",
    "  n = 50\n",
    "  for i in range(n) :\n",
    "    tokens[i+1].sort()\n",
    "\n",
    "  return tokens\n",
    "\n",
    "import re #regular Expression \n",
    "\n",
    "def MainIndex() :\n",
    "  #breaking string into word and applying caseFolding \n",
    "  tokens  = tokenization_caseFoldingg()\n",
    "  \n",
    "  invertedIndex = tokens\n",
    "  print(\"StopWordsRemoval->\")\n",
    "  invertedIndex = StopWordsRemoval(invertedIndex)\n",
    "  print(\"Okey\")\n",
    "  print(\"lemmatization->\")\n",
    "  invertedIndex = lemmatization(invertedIndex)\n",
    "  print(\"Okey\")\n",
    "  #invertedIndex = stemming(invertedIndex)\n",
    "  invertedIndex = sortIndex(invertedIndex)\n",
    "  save(invertedIndex,'invertedIndex')\n",
    "\n",
    "MainIndex()\n",
    "\n",
    "\"\"\"## Reading from Disk\"\"\"\n",
    "\n",
    "import json\n",
    "with open(\"GUI\\invertedIndex.json\", 'r') as ii:\n",
    "    Inverted_index = json.load(ii)\n",
    "\n",
    "\"\"\"## VECTOR SPACE MODEL\n",
    "\n",
    "### INDEXING\n",
    "\n",
    "#### TermFrequency\n",
    "\"\"\"\n",
    "\n",
    "def calTermFrequency(tokens,uniquelist) :\n",
    "  n = 50\n",
    "\n",
    "  vectorSpaceModel =  {}\n",
    "  \n",
    "  #InvertedIndex\n",
    "  for i in range(n) :\n",
    " \n",
    "    for Uword in uniquelist :\n",
    "      counter = 0 \n",
    "      counter = tokens[str(i+1)].count(Uword)\n",
    "      #print(\"here==\",Uword,counter,i)\n",
    "      if vectorSpaceModel.get(i+1) == None :\n",
    "        vectorSpaceModel.update({(i+1):[]})\n",
    "      vectorSpaceModel[i+1].append(counter)\n",
    "\n",
    "  return vectorSpaceModel\n",
    "\n",
    "def calTermFrequency_query(queryy,uniquelist) :\n",
    "  n = 50\n",
    "\n",
    "  query_tf =  {}\n",
    "  \n",
    "  #InvertedIndex\n",
    "\n",
    "  for Uword in uniquelist :\n",
    "    counter = 0 \n",
    "    counter = queryy.count(Uword)\n",
    "    # if counter >= 1 :\n",
    "    #   print(\"yes i am here \")\n",
    "    #print(\"here==\",Uword,counter,i)\n",
    "    if query_tf.get('query') == None :\n",
    "      query_tf.update({('query'):[]})\n",
    "    query_tf['query'].append(counter)\n",
    "\n",
    "  return query_tf\n",
    "\n",
    "\n",
    "\"\"\"#### Document Frequency\"\"\"\n",
    "\n",
    "def DocumentFrequency(tokens,uniquelist) :\n",
    "  df = []\n",
    "  for j in range(len(uniquelist)) :\n",
    "     counter = 0 \n",
    "     for i in range(50):\n",
    "       if tokens[i+1][j] != 0 :\n",
    "          counter = counter + 1\n",
    "      \n",
    "     df.append(counter)\n",
    "  \n",
    "  df = cal_idf(df,50)\n",
    "\n",
    "  return df\n",
    "\n",
    "\"\"\"#### TF IDF\"\"\"\n",
    "\n",
    "import math \n",
    "def cal_idf(df,n) :\n",
    "  for i in range(len(df)) :\n",
    "    idf = math.log(df[i],10) / n \n",
    "    #idf =  math.log(n/df[i],10)\n",
    "    df[i] = idf\n",
    "  return df\n",
    "\n",
    "def calculateTFIDF(vsm , uniquelt , df) :\n",
    "\n",
    "  for i in range(len(uniquelt)) :\n",
    "    \n",
    "    for j in range(50) :\n",
    "      vsm[j+1][i] = vsm[j+1][i] * df[i]\n",
    "\n",
    "  return vsm\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"#### main \"\"\"\n",
    "\n",
    "def MainVSM(tokens,uniquelist) :\n",
    "  \n",
    "  \n",
    "  #calculate the Unique list for VSM\n",
    "  for i in range(50) :\n",
    "    uniquelist = unique(tokens[str(i+1)],uniquelist)\n",
    " \n",
    "  uniquelist.sort()\n",
    "  \n",
    "  save(uniquelist,\"PUniqueList\")\n",
    "#   print(\"calTermFrequency->\")\n",
    "  VSM  = calTermFrequency(tokens,uniquelist)\n",
    "  print(\"Okey\")\n",
    "  save(VSM,\"TF\")\n",
    "#   print(\"DocumentFrequency->\")\n",
    "  df = DocumentFrequency(VSM , uniquelist)\n",
    "#   print(\"Okey\")\n",
    "#   print(\"calculateTFIDF->\")\n",
    "  VSM = calculateTFIDF(VSM,uniquelist,df)\n",
    "#   print(\"Okey\")\n",
    "  \n",
    "  save(VSM,\"VSM\")\n",
    "\n",
    "  return VSM\n",
    "#cal\n"
   ]
  },
  {
   "source": [
    "## Evaluation "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def EVALUATE() :\n",
    "    myfile = open('test.txt')\n",
    "    string  = myfile.read()\n",
    "    listt = list(string.split(\" \"))\n",
    "    index_query = 0\n",
    "    index_result= 0 \n",
    "    index_len = 0 \n",
    "    i = 0\n",
    "    for words in listt :\n",
    "        i += 1\n",
    "        if words == 'Query:' :\n",
    "            index_query = i\n",
    "        if words == '\\nResult:' :\n",
    "            index_result = i \n",
    "        if words == '\\nLength:' :\n",
    "            index_len = i \n",
    "    print(index_query,index_result,index_len)\n",
    "    query_input = []\n",
    "    result = []\n",
    "    leng = 0 \n",
    "    for i in range(index_query,index_result-1) :\n",
    "        query_input.append( listt[i] )\n",
    "    for i in range(index_result,index_len-1) :\n",
    "        result.append( listt[i] )\n",
    "\n",
    "    leng = listt[index_len] \n",
    "    print(query_input)\n",
    "    print(result)\n",
    "    print(leng)\n",
    "    return_DICT = {\n",
    "        'query':query_input,\n",
    "        'result':result,\n",
    "        'len':leng\n",
    "    }\n",
    "    return return_DICT\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[6, 11, 12, 16, 18, 22, 28, 41]\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "with open(\"GUI\\Result.json\", 'r') as ii:\n",
    "    result = json.load(ii)\n",
    "print(result)\n",
    "\n"
   ]
  },
  {
   "source": [
    "# VSM "
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Okey\n",
      "1 4 8\n",
      "['across', 'adventures']\n",
      "['9', '20', '44']\n",
      "3\n",
      "[9, 20, 44]\n",
      "Corret\n"
     ]
    }
   ],
   "source": [
    "uniquelist = []\n",
    "global_VSM = MainVSM(Inverted_index,uniquelist)\n",
    "\n",
    "\"\"\"## Query\"\"\"\n",
    "\n",
    "import json\n",
    "with open(\"GUI\\VSM.json\", 'r') as ii:\n",
    "    global_VSM = json.load(ii)\n",
    "\n",
    "\n",
    "with open(\"GUI\\pUniqueList.json\", 'r') as ij:\n",
    "    uniquelist = json.load(ij)\n",
    "\n",
    "\"\"\"### Preprocessing \"\"\"\n",
    "\n",
    "def PreprocessingQuery(queryy) :\n",
    "#   print(\"CaseFolding_query->\")\n",
    "  queryy = CaseFolding_query(queryy)\n",
    "#   print(\"Okey\")\n",
    "#   print(\"stopWordsRemoval_query->\")\n",
    "  queryy = stopWordsRemoval_query(queryy)\n",
    "#   print(\"Okey\")\n",
    "#   print(\"lemmatization_query->\")\n",
    "  queryy = lemmatization_query(queryy)\n",
    "#   print(\"Okey\")\n",
    "  \n",
    "  #queryy = stemming_query(queryy)\n",
    "  return queryy\n",
    "\n",
    "def CaseFolding_query(query) :\n",
    "  listt= []\n",
    "  for words in query :\n",
    "    listt.append(words.lower())\n",
    "  return listt\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"#### Lemmatization \n",
    "\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "\n",
    "def lemmatization_query(query) :\n",
    "  wordnet_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "  temp = []\n",
    "\n",
    "  for word in query :\n",
    "    newword = wordnet_lemmatizer.lemmatize(''.join(map(str, word)))\n",
    "    temp.append(newword)\n",
    "\n",
    "  return temp\n",
    "\n",
    "\"\"\"#### stopWordRemoval_query\"\"\"\n",
    "\n",
    "def stopWordsRemoval_query(queryy) :\n",
    "    myfile = open('Stopword-List.txt',encoding='utf-8')\n",
    "    string = myfile.read()\n",
    "    \n",
    "    #temp variable to store new words\n",
    "    temp = []\n",
    "    for word in queryy :\n",
    "      if word not in string.split() :\n",
    "        temp.append(word)\n",
    "    return temp\n",
    "\n",
    "\"\"\"#### stemming_query \"\"\"\n",
    "\n",
    "import nltk \n",
    "def stemming_query(queryy) :\n",
    "  from nltk.stem import PorterStemmer\n",
    "  porter = PorterStemmer()\n",
    "\n",
    "  temp = []\n",
    "  for word in queryy :\n",
    "    newword = porter.stem(''.join(map(str, word)))\n",
    "    temp.append(newword) \n",
    "  return temp\n",
    "\n",
    "\n",
    "\n",
    "\"\"\"### MAIN\"\"\"\n",
    "\n",
    "#input Queries\n",
    "def Inputt() :\n",
    "    print(\"Enter Query\")\n",
    "    querystring = input()\n",
    "    query= Convert(querystring)\n",
    "    return query\n",
    "\n",
    "#========string to list converter=============\n",
    "def Convert(string):\n",
    "    li = list(string.split(\" \"))\n",
    "    return li\n",
    "\n",
    "#=================cosine Functionality===========\n",
    "def magnitude(values) :\n",
    "  mag = 0\n",
    "  for i in range(len(values)) :\n",
    "    mag = mag + math.pow(values[i], 2)\n",
    "  \n",
    "  mag = math.sqrt(mag)\n",
    "  return mag\n",
    "\n",
    "\"\"\"#### DotProduct \"\"\"\n",
    "\n",
    "def DotProduct(doc,queryy) :\n",
    "  dotprod = 0\n",
    "  for i in range (len(doc)) :\n",
    "    dotprod = dotprod + doc[i]*queryy[i]\n",
    "\n",
    "  return dotprod\n",
    "#Taking input and storing in list\n",
    "# chalo='y'\n",
    "# while(chalo==\"Y\" or chalo=='y') :\n",
    "query = []\n",
    "ret_dict = EVALUATE()\n",
    "query = ret_dict['query']\n",
    "\n",
    "##PreProcessing the Query \n",
    "query = PreprocessingQuery(query)\n",
    "global_query_TF = calTermFrequency_query(query,uniquelist)\n",
    "save(global_query_TF ,\"query\")\n",
    "save(uniquelist,\"uni\")\n",
    "\"\"\"#############  MainRANKING  ################## \n",
    "\n",
    "\"\"\"\n",
    "# now i have 2 global varible \n",
    "# i) global_query_TF\n",
    "# ii) global_VSM\n",
    "COS_SIM = {}\n",
    "#storing the magnitude of query as it will use in all doc\n",
    "# print(\"magnitudeQuery->\")\n",
    "mag_query = magnitude(global_query_TF['query'])\n",
    "# print(\"Okey\")\n",
    "#calculating for each document \n",
    "for i in range(50) :\n",
    "    # print(\"DotProduct->\")\n",
    "    numerator = DotProduct(global_VSM[str(i+1)],global_query_TF['query'])\n",
    "    # print(\"Okey\")\n",
    "    # print(\"magnitudeDOC->\")\n",
    "    mag_doc =  magnitude(global_VSM[str(i+1)])\n",
    "    # print(\"Okey\")\n",
    "    # print(\"COSINE_SIM->\")\n",
    "    denominator = mag_doc * mag_query \n",
    "    try :\n",
    "        ans = numerator / denominator\n",
    "        COS_SIM.update({'SIMDOC{0}'.format(i+1):ans})\n",
    "        # print(\"OKEY\")\n",
    "    except :\n",
    "        ans = 0\n",
    "        # print(\"Not FOUND-OKEY\")\n",
    "\n",
    "#SIM\n",
    "queryResult = []\n",
    "# print(\"RANKGING->\")\n",
    "for i in range(len(COS_SIM)) :\n",
    "\n",
    "    val = COS_SIM['SIMDOC{0}'.format(i+1)]\n",
    "    if val > 0.005:\n",
    "        #print(i+1,val)\n",
    "        queryResult.append(i+1)\n",
    "save(queryResult,\"Result\")\n",
    "print(queryResult)\n",
    "flag = True \n",
    "for val_res in ret_dict['result'] :\n",
    "    if int(val_res) in queryResult :\n",
    "        flag == True\n",
    "    else :\n",
    "        flag = False \n",
    "if flag :\n",
    "    print(\"Corret\")\n",
    "else :\n",
    "    print(\"wrong\")\n",
    "#   print(\"\\n\\twant to continue?\\t Y/y or N/n\")\n",
    "#   chalo = input()\n",
    "  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}